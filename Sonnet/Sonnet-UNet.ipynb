{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3031b4b1",
   "metadata": {},
   "source": [
    "### We start by importing the required libraries for our U-Net implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aab4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import sonnet as snt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c2a12",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "### Pets dataset is used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501b514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   350  100   350    0     0    831      0 --:--:-- --:--:-- --:--:--   843\n",
      "100   185  100   185    0     0    288      0 --:--:-- --:--:-- --:--:--   288\n",
      "100  755M  100  755M    0     0  24.0M      0  0:00:31  0:00:30  0:00:01 24.8M  0:00:33  0:00:15  0:00:18 23.8M0:00:31  0:00:31 --:--:-- 23.1M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   355  100   355    0     0    987      0 --:--:-- --:--:-- --:--:--  1038\n",
      "100   185  100   185    0     0    264      0 --:--:-- --:--:-- --:--:--   264\n",
      "100 18.2M  100 18.2M    0     0  9770k      0  0:00:01  0:00:01 --:--:-- 22.3M\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "!curl -OL https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "!tar -xf images.tar.gz\n",
    "!tar -xf annotations.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e23378",
   "metadata": {},
   "source": [
    "### 1. Here we set up the paths for input images and target masks.\n",
    "### 2. It specifies image and mask preprocessing details such as size, number of classes, batch size, and epochs.\n",
    "### 3. It retrieves the paths of input images and target masks and sorts them.\n",
    "### 4. The dataset is split into training and test sets using an 80-20 split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f4b7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "# Set the directories for input images and target masks (annotations)\n",
    "input_dir = \"images/\"    # Directory containing input images\n",
    "target_dir = \"annotations/trimaps/\"    # Directory containing target masks\n",
    "\n",
    "# Image and Mask Preprocessing\n",
    "img_size = (160, 160)    # Set the desired size for input images and masks\n",
    "num_classes = 3    # Number of classes (foreground, background, and boundary)\n",
    "batch_size = 32    # Number of samples per batch during training\n",
    "epochs = 15    # Number of training epochs\n",
    "\n",
    "# Get the paths of input images and target masks, and sort them\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "# Divide input images and target masks into training and test sets using a 80-20 split ratio\n",
    "input_train_paths, input_test_paths, target_train_paths, target_test_paths = train_test_split(\n",
    "    input_img_paths, target_img_paths, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6660975",
   "metadata": {},
   "source": [
    "### 1. First we define a helper function to load and preprocess input images and target masks.\n",
    "### 2. The function reads the images from their respective paths and decodes them using TensorFlow operations.\n",
    "### 3. Input images are resized and normalized to the range [0, 1], while target masks are resized and binarized to [0, 1].\n",
    "### 4. TensorFlow datasets are created for training and testing, with images and masks preprocessed and batched accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a8f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load and preprocess images and masks\n",
    "def load_and_preprocess_image_and_mask(input_path, target_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the input image and target mask from their respective paths.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): File path to the input image.\n",
    "        target_path (str): File path to the target mask.\n",
    "\n",
    "    Returns:\n",
    "        input_image (tf.Tensor): Preprocessed input image as a TensorFlow tensor.\n",
    "        target_image (tf.Tensor): Preprocessed target mask as a TensorFlow tensor.\n",
    "    \"\"\"\n",
    "    # Read the input image from the file and decode it to a 3-channel (RGB) tensor\n",
    "    input_image = tf.image.decode_jpeg(tf.io.read_file(input_path), channels=3)\n",
    "    # Resize and normalize the input image to [0, 1]\n",
    "    input_image = tf.image.resize(input_image, img_size) / 255.0\n",
    "\n",
    "    # Read the target mask (annotation) from the file and decode it to a single-channel (grayscale) tensor\n",
    "    target_image = tf.image.decode_png(tf.io.read_file(target_path), channels=1)\n",
    "    # Resize and normalize the target mask to [0, 1]\n",
    "    target_image = tf.image.resize(target_image, img_size) // 128\n",
    "\n",
    "    return input_image, target_image\n",
    "\n",
    "# Create TensorFlow datasets for training and testing\n",
    "# For training dataset:\n",
    "# - Create a TensorFlow dataset from the lists of input_train_paths and target_train_paths.\n",
    "# - Use the load_and_preprocess_image_and_mask function to load and preprocess the images and masks.\n",
    "# - Batch the dataset, so each batch will contain 'batch_size' number of samples.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_train_paths, target_train_paths))\n",
    "train_dataset = train_dataset.map(load_and_preprocess_image_and_mask)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# For test dataset:\n",
    "# - Create a TensorFlow dataset from the lists of input_test_paths and target_test_paths.\n",
    "# - Use the load_and_preprocess_image_and_mask function to load and preprocess the images and masks.\n",
    "# - Batch the dataset, so each batch will contain 'batch_size' number of samples.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((input_test_paths, target_test_paths))\n",
    "test_dataset = test_dataset.map(load_and_preprocess_image_and_mask)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429363a2",
   "metadata": {},
   "source": [
    "### 1. Now we define a custom MaxPooling2D layer that inherits from Sonnet's snt.Module.\n",
    "### 2. The constructor sets the pooling window size, padding type, and layer name as specified.\n",
    "### 3. The __call__ method applies the MaxPooling2D operation on the input tensor using TensorFlow's tf.nn.max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80057e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MaxPooling2D layer for downsampling\n",
    "class MaxPooling2D(snt.Module):\n",
    "    def __init__(self, pool_size=(2, 2), padding=\"SAME\", name=\"max_pooling_2d\"):\n",
    "        \"\"\"\n",
    "        Custom MaxPooling2D layer constructor.\n",
    "\n",
    "        Parameters:\n",
    "            pool_size (tuple): The size of the pooling window as a tuple (height, width).\n",
    "            padding (str): The padding type to be applied during pooling. Default is \"SAME\".\n",
    "            name (str): The name of the layer. Default is \"max_pooling_2d\".\n",
    "        \"\"\"\n",
    "        super(MaxPooling2D, self).__init__(name=name)\n",
    "        self.pool_size = pool_size    # The size of the pooling window\n",
    "        self.padding = padding    # Padding type to be applied during pooling\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Apply MaxPooling2D operation on the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "            inputs (tf.Tensor): The input tensor to apply MaxPooling2D.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output tensor after MaxPooling2D operation.\n",
    "        \"\"\"\n",
    "        return tf.nn.max_pool2d(inputs, ksize=self.pool_size, strides=self.pool_size, padding=self.padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1dbde",
   "metadata": {},
   "source": [
    "### 1. Now we implement the U-Net model for multi-class segmentation.\n",
    "### 2. We define the U-Net model using TensorFlow and Sonnet (snt) framework.\n",
    "### 3. The model consists of encoder and decoder layers with skip connections for feature concatenation.\n",
    "### 4. The forward pass (__call__ method) through the U-Net model processes the input batch and produces the output segmentation map using the softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471d1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net Model Implementation\n",
    "class UNet(snt.Module):\n",
    "    def __init__(self, num_classes=3, name=\"UNet\"):\n",
    "        \"\"\"\n",
    "        U-Net model constructor.\n",
    "\n",
    "        Parameters:\n",
    "            num_classes (int): Number of output classes (foreground, background, etc.). Default is 3.\n",
    "            name (str): The name of the U-Net model. Default is \"UNet\".\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__(name=name)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_conv1 = snt.Conv2D(output_channels=64, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.encoder_conv2 = snt.Conv2D(output_channels=128, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.encoder_conv3 = snt.Conv2D(output_channels=256, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.encoder_conv4 = snt.Conv2D(output_channels=512, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.encoder_conv5 = snt.Conv2D(output_channels=1024, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.encoder_maxpool = MaxPooling2D()\n",
    "\n",
    "        # Decoder layers with skip connections\n",
    "        self.decoder_conv5 = snt.Conv2D(output_channels=512, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.decoder_conv4 = snt.Conv2D(output_channels=256, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.decoder_conv3 = snt.Conv2D(output_channels=128, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.decoder_conv2 = snt.Conv2D(output_channels=64, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.decoder_conv1 = snt.Conv2D(output_channels=num_classes, kernel_shape=(1, 1), padding=\"SAME\")\n",
    "\n",
    "        self.decoder_upsample5 = snt.Conv2DTranspose(output_channels=512, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.decoder_upsample4 = snt.Conv2DTranspose(output_channels=256, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.decoder_upsample3 = snt.Conv2DTranspose(output_channels=128, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "        self.decoder_upsample2 = snt.Conv2DTranspose(output_channels=64, kernel_shape=(3, 3), padding=\"SAME\")\n",
    "\n",
    "    def __call__(self, X_batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the U-Net model.\n",
    "\n",
    "        Parameters:\n",
    "            X_batch (tf.Tensor): The input batch of images.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output segmentation map.\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        enc1 = tf.nn.relu(self.encoder_conv1(X_batch))\n",
    "        enc2 = self.encoder_maxpool(tf.nn.relu(self.encoder_conv2(enc1)))\n",
    "        enc3 = self.encoder_maxpool(tf.nn.relu(self.encoder_conv3(enc2)))\n",
    "        enc4 = self.encoder_maxpool(tf.nn.relu(self.encoder_conv4(enc3)))\n",
    "        enc5 = self.encoder_maxpool(tf.nn.relu(self.encoder_conv5(enc4)))\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        dec5 = self.decoder_upsample5(enc5)\n",
    "        dec5 = tf.image.resize(dec5, size=enc4.shape[1:3], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        dec5 = tf.nn.relu(self.decoder_conv5(tf.concat([dec5, enc4], axis=-1)))\n",
    "\n",
    "        dec4 = self.decoder_upsample4(dec5)\n",
    "        dec4 = tf.image.resize(dec4, size=enc3.shape[1:3], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        dec4 = tf.nn.relu(self.decoder_conv4(tf.concat([dec4, enc3], axis=-1)))\n",
    "\n",
    "        dec3 = self.decoder_upsample3(dec4)\n",
    "        dec3 = tf.image.resize(dec3, size=enc2.shape[1:3], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        dec3 = tf.nn.relu(self.decoder_conv3(tf.concat([dec3, enc2], axis=-1)))\n",
    "\n",
    "        dec2 = self.decoder_upsample2(dec3)\n",
    "        dec2 = tf.image.resize(dec2, size=enc1.shape[1:3], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        dec2 = tf.nn.relu(self.decoder_conv2(tf.concat([dec2, enc1], axis=-1)))\n",
    "\n",
    "        dec1 = tf.nn.relu(self.decoder_conv1(dec2))\n",
    "\n",
    "        # Output segmentation map with softmax activation\n",
    "        output = tf.nn.softmax(dec1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87642dea",
   "metadata": {},
   "source": [
    "### 1. We are using the loss function using sparse categorical cross-entropy for multi-class segmentation.\n",
    "### 2. The training loop using stochastic gradient descent (SGD) optimizer updates the model's weights based on the calculated gradients.\n",
    "### 3. After each epoch, the loop evaluates the model's performance on the test dataset and prints the test loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bcdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started...\n"
     ]
    }
   ],
   "source": [
    "# Loss function for multi-class segmentation\n",
    "loss_func = tf.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Initialize U-Net model\n",
    "unet = UNet(num_classes=num_classes)\n",
    "\n",
    "# Training loop\n",
    "optimizer = snt.optimizers.SGD(learning_rate=1e-3)\n",
    "print(\"Training Started...\")\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, Y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = unet(X_batch)  # Make Predictions on Batch of Data\n",
    "            loss = loss_func(Y_batch, preds)  # Calculate Loss\n",
    "\n",
    "        params = unet.trainable_variables  # Retrieve Model Parameters\n",
    "        grads = tape.gradient(loss, params)  # Calculate Gradients\n",
    "\n",
    "        optimizer.apply(grads, params)  # Update Weights\n",
    "\n",
    "    # Calculate loss and accuracy for test dataset after each epoch\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    for X_batch, Y_batch in test_dataset:\n",
    "        preds = unet(X_batch)\n",
    "        test_losses.append(loss_func(Y_batch, preds))\n",
    "        test_accuracies.append(tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(Y_batch, preds)))\n",
    "\n",
    "    avg_test_loss = tf.reduce_mean(tf.convert_to_tensor(test_losses))\n",
    "    avg_test_accuracy = tf.reduce_mean(tf.convert_to_tensor(test_accuracies))\n",
    "\n",
    "    print(\"Epoch {}: Test Loss: {:.4f}, Test Accuracy: {:.4f}\".format(epoch + 1, avg_test_loss, avg_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78c58d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
